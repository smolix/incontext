%%%%%%%% ICML 2026 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2026} with \usepackage[nohyperref]{icml2026} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2026}

% For preprint, use
% \usepackage[preprint]{icml2026}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2026}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
% amsthm removed - alex.sty uses theorem package instead
\usepackage{alex}


% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

% THEOREMS are defined in alex.sty

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Deep Nets In Context}

\begin{document}

\twocolumn[
  \icmltitle{Deep Networks in Context --- Rates, Algorithms and Datastructures}

  % It is OKAY to include author information, even for blind submissions: the
  % style file will automatically remove it for you unless you've provided
  % the [accepted] option to the icml2026 package.

  % List of affiliations: The first argument should be a (short) identifier you
  % will use later to specify author affiliations Academic affiliations
  % should list Department, University, City, Region, Country Industry
  % affiliations should list Company, City, Region, Country

  % You can specify symbols, otherwise they are numbered in order. Ideally, you
  % should not use this facility. Affiliations will be numbered in order of
  % appearance and this is the preferred way.
  \icmlsetsymbol{equal}{*}

  \begin{icmlauthorlist}
    \icmlauthor{Firstname1 Lastname1}{equal,yyy}
    \icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
    \icmlauthor{Firstname3 Lastname3}{comp}
    \icmlauthor{Firstname4 Lastname4}{sch}
    \icmlauthor{Firstname5 Lastname5}{yyy}
    \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
    \icmlauthor{Firstname7 Lastname7}{comp}
    %\icmlauthor{}{sch}
    \icmlauthor{Firstname8 Lastname8}{sch}
    \icmlauthor{Firstname8 Lastname8}{yyy,comp}
    %\icmlauthor{}{sch}
    %\icmlauthor{}{sch}
  \end{icmlauthorlist}

  \icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
  \icmlaffiliation{comp}{Company Name, Location, Country}
  \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

  \icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
  \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

  % You may provide any keywords that you find helpful for describing your
  % paper; these are used to populate the "keywords" metadata in the PDF but
  % will not be shown in the document
  \icmlkeywords{Machine Learning, ICML}

  \vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column listing the
% affiliations and the copyright notice. The command takes one argument, which
% is text to display at the start of the footnote. The \icmlEqualContribution
% command is standard text for equal contribution. Remove it (just {}) if you
% do not need this facility.

% Use ONE of the following lines. DO NOT remove the command.
% If you have no special notice, KEEP empty braces:
\printAffiliationsAndNotice{}  % no special notice (required even if empty)
% Or, if applicable, use the standard equal contribution text:
% \printAffiliationsAndNotice{\icmlEqualContribution}

\begin{abstract}
  Deep Networks have demonstrated unreasonably good in-context learning ability. In this paper we provide theoretical guarantees for this phenomenon in sequence learning. Moreover, we demonstrate related effects for images and graphs. Lastly, we show that this leads to a unified explanation of attention, RAG, and context compression algorithms. Experiments confirm our theoretical guarantees and point to a rich area of new sequence models.
\end{abstract}

\section{Introduction}
\label{sec:intro}

\begin{itemize}
  \item In context learning is really popular. Very convenient alternative to training a specialized model from scratch.  
  \item One of the first observations was the Open AI paper where they used this for GPT-2 (look up the paper). 
  \item Pretty much the default for customizing LLMs for small numbers of training examples. Give some references. 
  \item Turkish MIT thesis looks at the problem in a phenomenological manner. Still seems to mystify the community. Even works for time series prediction (see Andrew Wilson's paper for time series using Llama). 
  \item Even recent ICL for audio models.
\end{itemize}

One of the key questions this paper aims to answer is why ICL works as well as it does and why the models exhibit ICL 'learning rates' that are very reminiscent of classical learning algorithms. We will see that this is not a coincidence but rather a consequence of the learning problems encoded in sequence models. Our work relies on observations that sequence models are regression estimators, as shown e.g.\ by \cite{SmoZha2021, Longhornpaper, SpeedAlwaysWins, WangFox25}. 

Moreover, our work uses theoretical guarantees of regret minimization guarantees for kernel density estimators and convex optimization and applies them to sequence models. These show that in many cases the regret in sequence models reduces at the rate of $O\rbr{l^{-\frac{1}2}}$ in the length $l$ of the context. 

Furnished with this insight, we extend our reasoning to structured data, such as images and graphs. In particular, we show that in context learning improves as a function of image patch size. Moreover, we illustrate how attention-enabled graph neural networks generalize label propagation algorithms and how the set transformer \cite{Lee18} implements what can be considered the purest version of ICL. 

Lastly, our insights aren't just analytical but also constructive - we use them to introduce a range of new algorithms that blend attention, sequence compression and RAG. While their detailed analysis is the subject of future work, it demonstrates the strength of our approach. 

\section{Related Work and Preliminaries}
\label{sec:related}

Maybe listing it all here is a bit too much but it'll set the context nicely for our work. It also serves the purpose of putting the Wang et al.\ 2025 paper into a bigger context. 

\paragraph{Unified DL and classical models}

\begin{itemize}
  \item Kernel density estimator from the Smola and Zhang ICML tutorial as a first example. 
  \item Longhorn paper
  \item Speed always wins setup
  \item Alex Wang's paper 
\end{itemize}

\paragraph{Regret minimization for sequences} 

Note that not all guarantees are for sequences. Some are for IID data. 
\begin{itemize}
  \item Bartlett, Hazan and Rakhlin results - https://www.stat.berkeley.edu/~bartlett/papers/bhr-aogd-07.pdf
  \item For IID data we have rates for kernels - https://proceedings.mlr.press/v70/jiang17b.html but note that the data isn't IID. 
  \item For online convex programming (i.e.\ SGD) we have the old Zinkevich result from 2003, namely https://www.cs.cmu.edu/~maz/publications/techconvex.pdf
\end{itemize}
It's worth finding out whether and what similar rates exist for the following methods. Note that this isn't for the \emph{learning algorithms} but rather for said models deployed on new data. 

\paragraph{Other datatypes}

We should briefly mention 
\begin{itemize}
  \item Label propagation algorithms on graphs
  \item Graph neural networks that have vertex update functions (so that's like unnormalized versions)
  \item Deep Sets and Set Transformer
  \item Vision Transformer (local context / full context)
  \item TabPFN is a great example where they learn missing entries (tabular data AutoML). 
\end{itemize}

\paragraph{Function classes}

There are actually a lot of different transformer variants:
\begin{itemize}
  \item Vanilla
  \item Grouped Query Attention
  \item NSA (Native Sparse Attention)
  \item History compression (Jegelka's paper)
  \item RAG for history
\end{itemize}

\section{In Context Learning}
\label{sec:icl}

\begin{itemize}
  \item Spell out the memorize and recall procedure
  \item Explain that since this is 'learning', it needs to follow the usual rules for learning. 
  \item Explain how this works for sequences. Note that this is \emph{not} IID, hence the regret bounds usually employed need to be used, rather than the IID convergence guarantees. This makes it hard for kernel density estimators but it will work for SSMs. 
\end{itemize}

Then decide to ignore the fact that this is not IID and just suggest that this should hold for sequence models in general. The tricky part is that while the kernel density estimate will probably work for the first layer, it won't work for subsequent ones, as the embeddings there are very much a function of data that's arrived so far. 

\subsection{Experiments}
\label{sec:icl-work}

Maybe we should pull the experiments for that aspect up here? 

\section{Datastructures}
\label{sec:datastructures}

We now need to expand our reasoning to nontrivial datastructures. IID doesn't make much sense there any longer but we can still do something useful: use the reasoning anyway. 

\begin{itemize}
  \item For images check what happens if we try filling in the blank image patch and we increase the size of the context for that. We should see a nice performance improvement as the amount of context provided grows. 
  \item For graphs, we can do the same thing - vertex attribute estimation with increasing graph context size. 
  \item For TabPFN we should be able to get decent learning rates. 
  \item Definitely good rates for Set Transformer models. Ideally let's download models from Huggingface and test it out. 
\end{itemize}

\section{Function Classes}
\label{sec:fun}

Given that all of these sequence models are just there to predict what happens next, we can look at the various memory compression and approximation strategies from a fresh point of view:

\begin{itemize}
  \item Grouped Query Attention --- basically just approximates larger groups of (key,value) sets as a single key and value. This works if shorter context doesn't matter that much at a distance. Reference He He's PhD work for this. 
  \item Low-dimensional Approximation --- this is one of the tricks from one of the DeepSeek papers. They run attention lower-dimensional. Again, this works as an approximation. 
  \item Retain (key,value) cache even though you swapped the model. E.g.\ this is used in ServiceNow Pipelined RL / Olmo 3. 
  \item Compress long-range content (rather than all groups). This is the NSA paper. 
  \item Compress long range overall --- Look up Steffi Jegelka's paper on using the Blelloch sum for long history. 
  \item Most extreme case is RAG. 
  \item Multi-edit and condense for kNN. Maybe we can recycle this here: https://cgm.cs.mcgill.ca/~godfried/teaching/pr-notes/dasarathy.pdf (the paper is quite soft tissue). 
  \item Can we design a hierarchical clustering / aggregation algorithm for the history? This should make lookups a lot cheaper (compressed prefill). 
\end{itemize}


% Acknowledgements should only appear in the accepted version.
\section*{Acknowledgements}

\textbf{Do not} include acknowledgements in the initial version of the paper
submitted for blind review.

If a paper is accepted, the final camera-ready version can (and usually should)
include acknowledgements.  Such acknowledgements should be placed at the end of
the section, in an unnumbered section that does not count towards the paper
page limit. Typically, this will include thanks to reviewers who gave useful
comments, to colleagues who contributed to the ideas, and to funding agencies
and corporate sponsors that provided financial support.

\section*{Impact Statement}

Authors are \textbf{required} to include a statement of the potential broader
impact of their work, including its ethical aspects and future societal
consequences. This statement should be in an unnumbered section at the end of
the paper (co-located with Acknowledgements -- the two may appear in either
order, but both must be before References), and does not count toward the paper
page limit. In many cases, where the ethical impacts and expected societal
implications are those that are well established when advancing the field of
Machine Learning, substantial discussion is not required, and a simple
statement such as the following will suffice:

``This paper presents work whose goal is to advance the field of Machine
Learning. There are many potential societal consequences of our work, none
which we feel must be specifically highlighted here.''

The above statement can be used verbatim in such cases, but we encourage
authors to think about whether there is content which does warrant further
discussion, as this statement will be apparent if the paper is later flagged
for ethics review.

You should not write

``This section has been left intentionally blank.''

The above statement would lead to blank faces and stares by the referees. 

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{bibfile}
\bibliographystyle{icml2026}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{You \emph{can} have an appendix here.}

You can have as much text here as you want. The main body must be at most $8$
pages long. For the final version, one more page can be added. If you want, you
can use an appendix like this one.

The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you
prefer a one-column appendix, or can be removed if you prefer a two-column
appendix.  Apart from this possible change, the style (font size, spacing,
margins, page numbering, etc.) should be kept the same as the main body.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}

% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
